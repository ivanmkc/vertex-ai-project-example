{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To achieve good shuffling, each label should be in its own file\n",
    "# # The function will take one sample from each label (i.e. file) and then shuffle them\n",
    "# def load_dataset(filenames: List[str], deserialization_function, number_of_files):\n",
    "#     parallel_reads = len(filenames)\n",
    "    \n",
    "#     files = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "#     ds = files.interleave(lambda x: tf.data.TFRecordDataset(x), \n",
    "#                           cycle_length=len(filenames), \n",
    "#                           block_length=1,\n",
    "#                           num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "#                           deterministic=False)\n",
    "\n",
    "#     ds = ds.flat_map(lambda x : deserialization_function(x, number_of_files))  # parse the record\n",
    "#     ds = ds.shuffle(parallel_reads, reshuffle_each_iteration=True)\n",
    "\n",
    "#     return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palette = [\n",
    "# '000000', # 0 No Data\n",
    "# 'a6cee3', # 1 Water\n",
    "# '1f78b4', # 2 Opaque Clouds\n",
    "# 'b2df8a', # 3 Trees and Shrubs\n",
    "# '33a02c', # 4 Built surface\n",
    "# 'fb9a99', # 5 Bridges and dams\n",
    "# 'e31a1c', # 6 Grass\n",
    "# 'fdbf6f', # 7 Plant/Ground Mix\n",
    "# 'ff7f00', # 8 Crops (other than Palm Plantations)\n",
    "# 'cab2d6', # 9 Palm Plantations\n",
    "# '6a3d9a', # 10 Flooded Vegetation\n",
    "# 'ffff99', # 11 Bare Ground and Sand\n",
    "# 'b15928', # 12 Snow and Ice\n",
    "# '000000'  # 13 Unknown\n",
    "# ]\n",
    "\n",
    "CLASSIFICATIONS = {\n",
    "  \"No data\": '000000',\n",
    "  \"Water\": 'a6cee3',\n",
    "  \"Opaque Clouds\": '1f78b4',\n",
    "  \"Trees and Shrubs\": 'b2df8a',\n",
    "  \"Built surface\": '33a02c',\n",
    "  \"Bridges and dams\": 'fb9a99',\n",
    "  \"Grass\": 'e31a1c',\n",
    "  \"Plant/Ground Mix\": 'fdbf6f',\n",
    "  \"Crops (other than Palm Plantations)\": 'ff7f00',\n",
    "  \"Palm Plantations\": 'cab2d6',\n",
    "  \"Flooded Vegetation\": '6a3d9a',\n",
    "  \"Bare Ground and Sand\": 'ffff99',\n",
    "  \"Snow and Ice\": 'b15928',\n",
    "  \"Unknown\": '000000'\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CLASSIFICATIONS)\n",
    "\n",
    "# Each tile is from 30cm WV3 satellite imagery, is 1024px x 1024px and is labelled twice.\n",
    "SCALE = 0.3\n",
    "PATCH_SIZE = 512\n",
    "\n",
    "INPUT_BANDS = ['R', 'G', 'B']\n",
    "LABELS_NAMES = ['label_1', 'label_2']\n",
    "FEATURES = INPUT_BANDS + LABELS_NAMES\n",
    "\n",
    "IMG_SIZE = [PATCH_SIZE, PATCH_SIZE, len(INPUT_BANDS)]\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SHAPE = [PATCH_SIZE, PATCH_SIZE]\n",
    "COLUMNS = [\n",
    "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
    "]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "\n",
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "  Returns:\n",
    "    A dictionary of tensors, keyed by feature name.\n",
    "  \"\"\"\n",
    "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "\n",
    "\n",
    "def to_tuple(tensor):\n",
    "  \"\"\"Function to convert a tensor to a tuple of (inputs, outputs).\n",
    "  Args:\n",
    "    tensor: A stacked tensor, with label last.\n",
    "  Returns:\n",
    "    A tuple of (inputs, outputs).\n",
    "  \"\"\"\n",
    "  # return tensor[:,:,:len(BANDS)], tensor[:,:,len(BANDS):]\n",
    "  return {\n",
    "    \"R\": tensor[:, :, 0],\n",
    "    \"G\": tensor[:, :, 1],\n",
    "    \"B\": tensor[:, :, 2]\n",
    "  }\n",
    "\n",
    "\n",
    "def flatten_patches(inputs):\n",
    "  \"\"\"Function to convert a dictionary of tensors to two stacked \n",
    "    tensors in HWC shape.\n",
    "  Args:\n",
    "    inputs: A dictionary of tensors, keyed by feature name.\n",
    "  Returns:\n",
    "    A tf.data.Dataset with two examaples in it.\n",
    "  \"\"\"\n",
    "  # inputsList = [inputs.get(key) for key in BANDS]\n",
    "  # label_1 = [inputs.get(LABELS_NAMES[0])]\n",
    "  # label_2 = [inputs.get(LABELS_NAMES[1])]\n",
    "  # stack1 = tf.stack(inputsList + label_1, axis=0)\n",
    "  # stack2 = tf.stack(inputsList + label_2, axis=0)\n",
    "  # # Convert from CHW to HWC\n",
    "  # return tf.data.Dataset.from_tensor_slices([\n",
    "  #   tf.transpose(stack1, [1, 2, 0]),\n",
    "  #   tf.transpose(stack2, [1, 2, 0]),\n",
    "  # ])\n",
    "  \n",
    "  bands = {key: inputs.get(key) for key in INPUT_BANDS}\n",
    "  \n",
    "  # return tf.data.Dataset.from_tensor_slices([{**bands, **{label_name: inputs.get(label_name)}} for label_name in LABELS_NAMES])\n",
    "  # return tf.data.Dataset.from_tensor_slices([{\"a\": \"A\"}, {\"b\": \"B\"}]) #tf.data.Dataset.from_tensor_slices((bands, bands))\n",
    "  # return [{**bands, **{label_name: inputs.get(label_name)}} for label_name in LABELS_NAMES]\n",
    "  return {**bands, **{LABELS_NAMES[0]: inputs.get(LABELS_NAMES[0])}}\n",
    "\n",
    "def preprocess(values: Dict[str, tf.Tensor]) -> Tuple[Dict[str, tf.Tensor], tf.Tensor]:\n",
    "  # Create a dictionary of band values.\n",
    "  inputs = {name: values[name] for name in INPUT_BANDS}\n",
    "\n",
    "  # Convert the labels into one-hot encoded vectors.\n",
    "  outputs = tf.one_hot(tf.cast(values[\"label_1\"], tf.uint8), len(CLASSIFICATIONS))\n",
    "  return (inputs, outputs)\n",
    "\n",
    "def get_dataset(glob):\n",
    "  \"\"\"\"\"\"\n",
    "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "  # dataset = dataset.map(flatten_patches)\n",
    "  # dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
    "  dataset = dataset.map(preprocess)\n",
    "\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_datasets(pattern):\n",
    "    \"\"\"\"\"\"\n",
    "    glob = tf.io.gfile.glob(pattern)\n",
    "    size = len(glob)\n",
    "    print(f\"size: {size}\")\n",
    "    train_size = int(0.8*size)\n",
    "    shuffled = tf.random.shuffle(glob)\n",
    "    train_files = shuffled[:train_size]\n",
    "    test_files = shuffled[train_size:]\n",
    "    training = get_dataset(train_files) #.take(24)\n",
    "    training = training.batch(16)\n",
    "    # training = training.shuffle(2048).repeat()\n",
    "    testing = get_dataset(test_files) #.take(24)\n",
    "    testing = testing.batch(16)\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 2014\n"
     ]
    }
   ],
   "source": [
    "pattern = \"gs://ivanmkc-palm-data-2/high-res-patches/labels_*.tfrecord.gz\"\n",
    "# pattern = \"gs://ivanmkc-palm-data-2/high-res-patches/labels_983.tfrecord.gz\"\n",
    "\n",
    "training_dataset, testing_dataset = get_datasets(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(testing_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in training_dataset.take(1).as_numpy_iterator():\n",
    "#     print(item)\n",
    "\n",
    "# items = list(testing_dataset.take(2).as_numpy_iterator())\n",
    "# item = items[0]\n",
    "\n",
    "# image = np.stack([item['R'], item['G'], item['B']], 2)\n",
    "# mask = np.stack([item['label_1']], 2)\n",
    "\n",
    "# np.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def display_image_mask(image, mask):\n",
    "#   plt.figure(figsize=(15, 15))\n",
    "\n",
    "#   title = ['Input Image', 'True Mask']\n",
    "\n",
    "#   display_list = [image, mask]\n",
    "\n",
    "#   for i in range(len(display_list)):\n",
    "#     plt.subplot(1, len(display_list), i+1)\n",
    "#     plt.title(title[i])\n",
    "#     plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "#     plt.axis('off')\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_image_mask(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, value in item.items():\n",
    "#     print(f\"{name}: {value.dtype.name} {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(item['label_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model(dataset: tf.data.TFRecordDataset, input_layers: Dict[str, tf.keras.Input], classifications: Dict[str, str]) -> tf.keras.Model:\n",
    "    # Adapt the Normalization layer with the training dataset.\n",
    "    normalization = tf.keras.layers.Normalization(name=\"Normalize\")\n",
    "    normalization.adapt(\n",
    "        dataset.map(\n",
    "            lambda inputs, _: tf.stack([inputs[name] for name in input_bands], axis=-1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Define the Fully Convolutional Network.\n",
    "    fcn_model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(None, None, len(input_layers)), name=\"Inputs\"),\n",
    "        normalization,\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", name=\"Conv2D\"),\n",
    "        tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=5, activation=\"relu\", name=\"Deconv2D\"),\n",
    "        tf.keras.layers.Dense(len(classifications), activation=\"softmax\", name=\"LandCover\"),\n",
    "    ], name=\"FullyConvolutionalNetwork\")\n",
    "    \n",
    "    fcn_model.summary()\n",
    "    tf.keras.utils.plot_model(fcn_model, show_shapes=True)\n",
    "\n",
    "    return fcn_model\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model_unet(dataset: tf.data.TFRecordDataset, input_layers: Dict[str, tf.keras.Input], classifications: Dict[str, str]) -> tf.keras.Model:\n",
    "    inputs = tf.keras.Input(shape=(None, None, len(input_layers)), name=\"Inputs\")\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(len(classifications), 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "    # model = model(inputs)\n",
    "    # return model\n",
    "\n",
    "    # return inner_model\n",
    "\n",
    "# Define the input dictionary layers.\n",
    "input_layers = {\n",
    "    name: tf.keras.Input(shape=(None, None, 1), name=name)\n",
    "    for name in INPUT_BANDS\n",
    "}\n",
    "\n",
    "inner_model = get_model_unet(dataset=training_dataset, input_layers=input_layers, classifications=CLASSIFICATIONS)\n",
    "\n",
    "# Model wrapper that takes an input dictionary and feeds it to the FCN.\n",
    "inputs_stack = tf.keras.layers.concatenate(input_layers.values(), name=\"Stack\")\n",
    "\n",
    "model = tf.keras.Model(input_layers, inner_model(inputs_stack))\n",
    "\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n",
    "    ''' \n",
    "    Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "    Assumes the `channels_last` format.\n",
    "  \n",
    "    # Arguments\n",
    "        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n",
    "        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n",
    "        epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "    \n",
    "    # References\n",
    "        V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation \n",
    "        https://arxiv.org/abs/1606.04797\n",
    "        More details on Dice loss formulation \n",
    "        https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "        \n",
    "        Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "    '''\n",
    "    \n",
    "    # skip the batch and class axis for calculating Dice score\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1)) \n",
    "    numerator = 2. * np.sum(y_pred * y_true, axes)\n",
    "    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n",
    "    \n",
    "    return 1 - np.mean((numerator + epsilon) / (denominator + epsilon)) # average over classes and batch\n",
    "    # thanks @mfernezir for catching a bug in an earlier version of this implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def DiceBCELoss(targets, inputs, smooth=1e-6):\n",
    "    #flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    intersection = K.sum(K.dot(targets, inputs))\n",
    "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
    "    Dice_BCE = BCE + dice_loss\n",
    "    \n",
    "    return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    # loss=DiceBCELoss,\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)],\n",
    ")\n",
    "\n",
    "# https://lars76.github.io/2018/09/27/loss-functions-for-segmentation.html\n",
    "# https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook#Usage-Tips\n",
    "# https://www.jeremyjordan.me/semantic-segmentation/#loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "     84/Unknown - 114s 1s/step - loss: 2.8445 - accuracy: 0.4596 - mean_io_u_4: 0.4035"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "history = model.fit(\n",
    "    training_dataset.shuffle(10),\n",
    "    validation_data=testing_dataset,\n",
    "    epochs=15,\n",
    ")\n",
    "\n",
    "# Save it as files.\n",
    "model.save(\"model-unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"mean_io_u_8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"model-mean-iou\")\n",
    "\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def render_classifications(values: np.ndarray, palette: List[str]) -> np.ndarray:\n",
    "  # Create a color map from a hex color palette.\n",
    "  xs = np.linspace(0, len(palette), 256)\n",
    "  indices = np.arange(len(palette))\n",
    "  color_map = np.array([\n",
    "        np.interp(xs, indices, [int(c[0:2], 16) for c in palette]),  # red\n",
    "        np.interp(xs, indices, [int(c[2:4], 16) for c in palette]),  # green\n",
    "        np.interp(xs, indices, [int(c[4:6], 16) for c in palette]),  # blue\n",
    "  ]).astype(np.uint8).transpose()\n",
    "\n",
    "  color_indices = (values / len(palette) * 255).astype(np.uint8)\n",
    "  return np.take(color_map, color_indices, axis=0)\n",
    "\n",
    "def render_landcover(patch: np.ndarray) -> np.ndarray:\n",
    "  return render_classifications(patch, list(CLASSIFICATIONS.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_legend():\n",
    "  reset_color = \"\\u001b[0m\"\n",
    "  colored = lambda red, green, blue: f\"\\033[48;2;{red};{green};{blue}m\"\n",
    "  for name, color in CLASSIFICATIONS.items():\n",
    "    red   = int(color[0:2], 16)\n",
    "    green = int(color[2:4], 16)\n",
    "    blue  = int(color[4:6], 16)\n",
    "    print(f\"{colored(red, green, blue)}   {reset_color} {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(testing_dataset.take(10).as_numpy_iterator())\n",
    "patches, masks = batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = [12, 12]     # figure size, inches\n",
    "\n",
    "def predict_and_compare(patch_batch: Dict[str, np.ndarray], mask_batch: Dict[str, np.ndarray]):\n",
    "    # fig.add_subplot(len(patch_batch), 3, i)\n",
    "    batch_size = 3 # len(patch_batch['R'])\n",
    "    fig, ax = plt.subplots(nrows=batch_size, ncols=3, figsize=figsize, dpi=100)\n",
    "    fig.tight_layout()\n",
    "    display_legend()\n",
    "        \n",
    "    ax[0, 0].set_title('Image (RGB)')\n",
    "    ax[0, 1].set_title('Actual mask')\n",
    "    ax[0, 2].set_title('Predicted mask')\n",
    "        \n",
    "    for patch_index in range(batch_size):\n",
    "        prediction = model.predict(patch_batch)[patch_index]\n",
    "        outputs = np.argmax(prediction, axis=-1).astype(np.uint8)\n",
    "        # print(f\"outputs: {outputs.dtype} {outputs.shape}\")\n",
    "        \n",
    "        # Plot image\n",
    "        image = (np.stack([patches['R'][patch_index], patches['G'][patch_index], patches['B'][patch_index]], axis=2)*255).astype(np.uint8)\n",
    "        ax[patch_index, 0].imshow(image)\n",
    "            \n",
    "        # Plot actual masks\n",
    "        mask = np.argmax(masks[patch_index, :, :, :], axis=-1).astype(np.uint8)\n",
    "        ax[patch_index, 1].imshow(render_landcover(mask))\n",
    "                    \n",
    "        # Plot predictions\n",
    "        ax[patch_index, 2].imshow(render_landcover(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_compare(patch_batch=patches, mask_batch=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_distributions = []\n",
    "\n",
    "for _, mask_batch in testing_dataset.take(10).as_numpy_iterator():\n",
    "    for batch_index in range(0, len(mask_batch)):\n",
    "        mask_distributions.append({key: mask_batch[batch_index, :, :, classification_index].sum()/(512*512) for classification_index, key in enumerate(CLASSIFICATIONS.keys())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(mask_distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f6280b32b24df617ee52bc9a54f5fd43d751c5bddace2cefb1417d94c3321df"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
