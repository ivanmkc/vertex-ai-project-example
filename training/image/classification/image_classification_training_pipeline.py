import training_pipeline
from google_cloud_pipeline_components import aiplatform as gcc_aip
from google.cloud import aiplatform
import kfp
from kfp.v2 import dsl
from kfp.dsl import importer_node
from kfp.v2.dsl import (
    ClassificationMetrics,
    Dataset,
    Input,
    Metrics,
    Model,
    Output,
    component,
)

from typing import Any, Callable, NamedTuple


class ImageClassificationTrainingPipeline(training_pipeline.TrainingPipeline):
    @component(
        base_image="gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest",
        output_component_file="tables_eval_component.yaml",
        packages_to_install=["google-cloud-aiplatform"],
    )
    def classify_model_eval_metrics(
        project_id: str,
        # location: str,
        api_endpoint: str,  # "us-central1-aiplatform.googleapis.com",
        thresholds_dict_str: str,
        model: Input[Model],
        metrics: Output[Metrics],
        metrics_classification: Output[ClassificationMetrics],
    ) -> NamedTuple("Outputs", [("dep_decision", str)]):  # Return parameter.

        """This function renders evaluation metrics for an AutoML Image classification model.
        It retrieves the classification model evaluation generated by the AutoML Image training
        process, does some parsing, and uses that info to render the ROC curve and confusion matrix
        for the model. It also uses given metrics threshold information and compares that to the
        evaluation results to determine whether the model is sufficiently accurate to deploy.
        """
        import json
        import logging

        from google.cloud import aiplatform

        # Fetch model eval info
        def get_eval_info(client, model_name):
            from google.protobuf.json_format import MessageToDict

            response = client.list_model_evaluations(parent=model_name)
            metrics_list = []
            metrics_string_list = []
            for evaluation in response:
                print("model_evaluation")
                print(" name:", evaluation.name)
                print(" metrics_schema_uri:", evaluation.metrics_schema_uri)
                metrics = MessageToDict(evaluation._pb.metrics)
                for metric in metrics.keys():
                    logging.info("metric: %s, value: %s", metric, metrics[metric])
                metrics_str = json.dumps(metrics)
                metrics_list.append(metrics)
                metrics_string_list.append(metrics_str)

            return (
                evaluation.name,
                metrics_list,
                metrics_string_list,
            )

        # Use the given metrics threshold(s) to determine whether the model is
        # accurate enough to deploy.
        def classification_thresholds_check(metrics_dict, thresholds_dict):
            for k, v in thresholds_dict.items():
                logging.info("k {}, v {}".format(k, v))
                if k in ["auRoc", "auPrc"]:  # higher is better
                    if metrics_dict[k] < v:  # if under threshold, don't deploy
                        logging.info(
                            "{} < {}; returning False".format(metrics_dict[k], v)
                        )
                        return False
            logging.info("threshold checks passed.")
            return True

        def log_metrics(metrics_list, metrics_classification):
            test_confusion_matrix = metrics_list[0]["confusionMatrix"]
            logging.info("rows: %s", test_confusion_matrix["rows"])

            # log the ROC curve
            fpr = []
            tpr = []
            thresholds = []
            for item in metrics_list[0]["confidenceMetrics"]:
                fpr.append(item.get("falsePositiveRate", 0.0))
                tpr.append(item.get("recall", 0.0))
                thresholds.append(item.get("confidenceThreshold", 0.0))
            print(f"fpr: {fpr}")
            print(f"tpr: {tpr}")
            print(f"thresholds: {thresholds}")
            metrics_classification.log_roc_curve(fpr, tpr, thresholds)

            # log the confusion matrix
            annotations = []
            for item in test_confusion_matrix["annotationSpecs"]:
                annotations.append(item["displayName"])
            logging.info("confusion matrix annotations: %s", annotations)
            metrics_classification.log_confusion_matrix(
                annotations,
                test_confusion_matrix["rows"],
            )

            # log textual metrics info as well
            for metric in metrics_list[0].keys():
                if metric != "confidenceMetrics":
                    val_string = json.dumps(metrics_list[0][metric])
                    metrics.log_metric(metric, val_string)
            # metrics.metadata["model_type"] = "AutoML Image classification"

        logging.getLogger().setLevel(logging.INFO)
        aiplatform.init(project=project_id)
        # extract the model resource name from the input Model Artifact
        model_resource_path = model.uri.replace("aiplatform://v1/", "")
        logging.info("model path: %s", model_resource_path)

        client_options = {"api_endpoint": api_endpoint}
        # Initialize client that will be used to create and send requests.
        client = aiplatform.gapic.ModelServiceClient(client_options=client_options)

        eval_name, metrics_list, metrics_str_list = get_eval_info(
            client, model_resource_path
        )
        logging.info("got evaluation name: %s", eval_name)
        logging.info("got metrics list: %s", metrics_list)
        log_metrics(metrics_list, metrics_classification)

        thresholds_dict = json.loads(thresholds_dict_str)
        deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)
        if deploy:
            dep_decision = "true"
        else:
            dep_decision = "false"
        logging.info("deployment decision is %s", dep_decision)

        return (dep_decision,)

    def create_pipeline(self, project: str, pipeline_root: str) -> Callable[..., Any]:
        @kfp.dsl.pipeline(name=self.pipeline_name, pipeline_root=pipeline_root)
        def pipeline(
            api_endpoint: str = "us-central1-aiplatform.googleapis.com",
            thresholds_dict_str: str = '{"auPrc": 0.95}',
        ):
            importer = importer_node.importer(
                artifact_uri=self.annotation_dataset_uri,
                artifact_class=Dataset,
                reimport=False,
            )

            training_op = gcc_aip.AutoMLImageTrainingJobRunOp(
                project=project,
                display_name="train-iris-automl-mbsdk-1",
                prediction_type="classification",
                model_type="CLOUD",
                base_model=None,
                dataset=importer.output,
                model_display_name="iris-classification-model-mbsdk",
                training_fraction_split=0.6,
                validation_fraction_split=0.2,
                test_fraction_split=0.2,
                budget_milli_node_hours=8000,
            )

            model_eval_task = ImageClassificationTrainingPipeline.classify_model_eval_metrics(
                project_id=project,
                # gcp_region,
                api_endpoint=api_endpoint,
                thresholds_dict_str=thresholds_dict_str,
                model=training_op.outputs["model"],
            )

            with dsl.Condition(
                model_eval_task.outputs["dep_decision"] == "true",
                name="deploy_decision",
            ):

                deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841
                    model=training_op.outputs["model"],
                )

        return pipeline


class ImageClassificationTrainingPipelineFlowers(ImageClassificationTrainingPipeline):
    id = "Image Classification Flowers"
    annotation_dataset_uri: str = "aiplatform://v1/projects/1012616486416/locations/us-central1/datasets/7601275726536376320"


class ImageClassificationTrainingPipelineFruit(ImageClassificationTrainingPipeline):
    id = "Image Classification Fruit"
    annotation_dataset_uri: str = "aiplatform://v1/projects/1012616486416/locations/us-central1/datasets/7601275726536376320"
