from typing import Any, Callable, Dict, NamedTuple
import abc
from kfp.dsl import importer_node

from training.common.managed_dataset_pipeline import ManagedDatasetPipeline
from google_cloud_pipeline_components import aiplatform as gcc_aip
import kfp
from kfp.v2 import dsl
from kfp.v2.dsl import (
    ClassificationMetrics,
    Dataset,
    Input,
    Metrics,
    Model,
    Output,
    component,
)

import json


class ConditionalDeploymentManagedDatasetPipeline(ManagedDatasetPipeline):
    @property
    @abc.abstractmethod
    def thresholds_dict(self) -> Dict[str, float]:  # e.g. {"auPrc": 0.95}
        pass

    @abc.abstractmethod
    def create_training_op(self, project: str, dataset: Dataset) -> Callable:
        pass

    @component(
        base_image="gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest",
        output_component_file="tables_eval_component.yaml",
        packages_to_install=["google-cloud-aiplatform"],
    )
    def classify_model_eval_metrics(
        project_id: str,
        # location: str,
        api_endpoint: str,  # "us-central1-aiplatform.googleapis.com",
        thresholds_dict_str: str,
        model: Input[Model],
        metrics: Output[Metrics],
        metrics_classification: Output[ClassificationMetrics],
    ) -> NamedTuple("Outputs", [("dep_decision", str)]):  # Return parameter.

        """This function renders evaluation metrics for an AutoML Image classification model.
        It retrieves the classification model evaluation generated by the AutoML Image training
        process, does some parsing, and uses that info to render the ROC curve and confusion matrix
        for the model. It also uses given metrics threshold information and compares that to the
        evaluation results to determine whether the model is sufficiently accurate to deploy.
        """
        import json
        import logging

        from google.cloud import aiplatform

        # Fetch model eval info
        def get_eval_info(client, model_name):
            from google.protobuf.json_format import MessageToDict

            response = client.list_model_evaluations(parent=model_name)
            metrics_list = []
            metrics_string_list = []
            for evaluation in response:
                print("model_evaluation")
                print(" name:", evaluation.name)
                print(" metrics_schema_uri:", evaluation.metrics_schema_uri)
                metrics = MessageToDict(evaluation._pb.metrics)
                for metric in metrics.keys():
                    logging.info("metric: %s, value: %s", metric, metrics[metric])
                metrics_str = json.dumps(metrics)
                metrics_list.append(metrics)
                metrics_string_list.append(metrics_str)

            # TODO: This errors out when evaluation is not assigned in the for loop
            return (
                evaluation.name,
                metrics_list,
                metrics_string_list,
            )

        # Use the given metrics threshold(s) to determine whether the model is
        # accurate enough to deploy.
        thresholds_dict = json.loads(thresholds_dict_str)

        # TODO: Throw error if evaluation doesn't contain the values required in thresholds_dict

        def classification_thresholds_check(metrics_dict, thresholds_dict):
            for k, v in thresholds_dict.items():
                logging.info("k {}, v {}".format(k, v))
                if k in ["auRoc", "auPrc"]:  # higher is better
                    if metrics_dict[k] < v:  # if under threshold, don't deploy
                        logging.info(
                            "{} < {}; returning False".format(metrics_dict[k], v)
                        )
                        return False
            logging.info("threshold checks passed.")
            return True

        def log_metrics(metrics_list, metrics_classification):
            test_confusion_matrix = metrics_list[0]["confusionMatrix"]
            logging.info("rows: %s", test_confusion_matrix["rows"])

            # log the ROC curve
            fpr = []
            tpr = []
            thresholds = []
            for item in metrics_list[0]["confidenceMetrics"]:
                fpr.append(item.get("falsePositiveRate", 0.0))
                tpr.append(item.get("recall", 0.0))
                thresholds.append(item.get("confidenceThreshold", 0.0))
            print(f"fpr: {fpr}")
            print(f"tpr: {tpr}")
            print(f"thresholds: {thresholds}")
            metrics_classification.log_roc_curve(fpr, tpr, thresholds)

            # log the confusion matrix
            annotations = []
            for item in test_confusion_matrix["annotationSpecs"]:
                annotations.append(item["displayName"])
            logging.info("confusion matrix annotations: %s", annotations)
            metrics_classification.log_confusion_matrix(
                annotations,
                test_confusion_matrix["rows"],
            )

            # log textual metrics info as well
            for metric in metrics_list[0].keys():
                if metric != "confidenceMetrics":
                    val_string = json.dumps(metrics_list[0][metric])
                    metrics.log_metric(metric, val_string)
            # metrics.metadata["model_type"] = "AutoML Image classification"

        logging.getLogger().setLevel(logging.INFO)
        aiplatform.init(project=project_id)
        # extract the model resource name from the input Model Artifact
        model_resource_path = model.uri.replace("aiplatform://v1/", "")
        logging.info("model path: %s", model_resource_path)

        client_options = {"api_endpoint": api_endpoint}
        # Initialize client that will be used to create and send requests.
        client = aiplatform.gapic.ModelServiceClient(client_options=client_options)

        eval_name, metrics_list, metrics_str_list = get_eval_info(
            client, model_resource_path
        )
        logging.info("got evaluation name: %s", eval_name)
        logging.info("got metrics list: %s", metrics_list)
        log_metrics(metrics_list, metrics_classification)

        deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)
        if deploy:
            dep_decision = "true"
        else:
            dep_decision = "false"
        logging.info("deployment decision is %s", dep_decision)

        return (dep_decision,)

    def create_pipeline(
        self, project: str, location: str, pipeline_root: str
    ) -> Callable[..., Any]:
        @kfp.dsl.pipeline(name=self.name, pipeline_root=pipeline_root)
        def pipeline(
            api_endpoint: str = "us-central1-aiplatform.googleapis.com",
        ):
            importer = importer_node.importer(
                artifact_uri=self.managed_dataset_uri,
                artifact_class=Dataset,
                reimport=False,
            )

            training_op = self.create_training_op(
                project=project, pipeline_root=pipeline_root, dataset=importer.output
            )

            model_eval_task = ConditionalDeploymentManagedDatasetPipeline.classify_model_eval_metrics(
                project_id=project,
                # gcp_region,
                api_endpoint=api_endpoint,
                thresholds_dict_str=json.dumps(self.thresholds_dict),
                model=training_op.outputs["model"],
            )

            with dsl.Condition(
                model_eval_task.outputs["dep_decision"] == "true",
                name="deploy_decision",
            ):

                deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841
                    model=training_op.outputs["model"],
                )

        return pipeline
